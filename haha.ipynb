{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5])\n",
      "torch.Size([4, 5, 4, 4])\n",
      "tensor([[ 3.,  3.,  4.,  5.,  3.],\n",
      "        [ 6.,  0.,  7.,  8.,  8.],\n",
      "        [ 0.,  9., 10., 11.,  3.],\n",
      "        [12., 13.,  0.,  0.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "import torch\n",
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "W=np.array([[3,3,4,5,3],\n",
    "[6,0,7,8,8],\n",
    "[0,9,10,11,3],\n",
    "[12,13,0,0,1]])\n",
    "\n",
    "\n",
    "Pin2=np.array([[0,0,1],\n",
    "[1,0,0],\n",
    "[0,1,0],\n",
    "[1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "C=np.array([[0,19,5],\n",
    "[7,0,3],\n",
    "[1,11,0]])\n",
    "\n",
    "Pout=np.array([[0,1,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "E=torch.from_numpy(E).float()\n",
    "W=torch.from_numpy(W).float()\n",
    "Pin=torch.from_numpy(Pin).float()\n",
    "Pin2=torch.from_numpy(Pin2).float()\n",
    "Pout=torch.from_numpy(Pout).float()\n",
    "C=torch.from_numpy(C).float()\n",
    "\n",
    "\n",
    "def compress(X,c=1):\n",
    "    c=1-c\n",
    "    if c==1:\n",
    "        return X\n",
    "    dout = X.shape[1]\n",
    "    \n",
    "    # Calculate L1 norm for each output channel\n",
    "    if len(X.shape)==2:\n",
    "        Xvalues = torch.abs(X)\n",
    "    elif len(X.shape)==4:\n",
    "        Xvalues = torch.sum(torch.abs(X), dim=(2, 3))\n",
    "\n",
    "    print(Xvalues.shape)\n",
    "    \n",
    "    # Calculate how many channels to prune\n",
    "    k = int(dout * c)\n",
    "\n",
    "    x, i = torch.topk(Xvalues, k, dim=1)\n",
    "    mask = torch.zeros_like(Xvalues,dtype=torch.int)\n",
    "    src = torch.ones_like(i,dtype=torch.int)\n",
    "    mask.scatter_(1, i, src)\n",
    "    #print(Xvalues,x,i)\n",
    "    #print(mask)\n",
    "    if len(X.shape)==2:\n",
    "        return mask*X\n",
    "    else:\n",
    "        #print(mask.shape,X.shape)\n",
    "        #print(mask)\n",
    "        mask=mask.unsqueeze(2)\n",
    "        mask=mask.unsqueeze(3)\n",
    "        mask=mask.repeat((1,1,X.shape[2],X.shape[3]))\n",
    "        return mask*X\n",
    "def fakekernel(t,k1=4,k2=4):\n",
    "    t=t.unsqueeze(2)\n",
    "    t=t.unsqueeze(3)\n",
    "    t=t.repeat((1,1,k1,k2))\n",
    "    return t\n",
    "print(W.shape)\n",
    "t=fakekernel(W)\n",
    "print(t.shape)\n",
    "print(compress(W,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intersection points: [array([0. , 0.5, 0.5]), array([1. , 0.5, 0.5])]\n",
      "2D Intersection points: [array([0.5, 0. ]), array([1. , 0.5])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def line_box_intersection(line_start, line_end, box_min, box_max):\n",
    "    \"\"\"\n",
    "    Finds the intersection points of a line segment and an axis-aligned bounding box (AABB).\n",
    "\n",
    "    Args:\n",
    "        line_start (np.array): The start point of the line (e.g., np.array([x, y, z])).\n",
    "        line_end (np.array): The end point of the line (e.g., np.array([x, y, z])).\n",
    "        box_min (np.array): The minimum corner of the box (e.g., np.array([xmin, ymin, zmin])).\n",
    "        box_max (np.array): The maximum corner of the box (e.g., np.array([xmax, ymax, zmax])).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of intersection points (up to two points).\n",
    "    \"\"\"\n",
    "    #thanks Google's AI\n",
    "    line_dir = line_end - line_start\n",
    "    # Pre-calculate inverse direction components to replace division with multiplication (faster)\n",
    "    # Handle cases where a component is zero\n",
    "    inv_dir = np.array([1/line_dir[i] if line_dir[i] != 0 else np.inf for i in range(len(line_dir))])\n",
    "\n",
    "    t_min = -np.inf\n",
    "    t_max = np.inf\n",
    "    \n",
    "    intersection_points = []\n",
    "\n",
    "    for i in range(len(line_start)): # Iterate over x, y, z dimensions\n",
    "        t1 = (box_min[i] - line_start[i]) * inv_dir[i]\n",
    "        t2 = (box_max[i] - line_start[i]) * inv_dir[i]\n",
    "\n",
    "        # Ensure t1 is the smaller intersection parameter\n",
    "        if t1 > t2:\n",
    "            t1, t2 = t2, t1\n",
    "\n",
    "        # Update the overall t_min and t_max for the intersection interval\n",
    "        t_min = max(t_min, t1)\n",
    "        t_max = min(t_max, t2)\n",
    "\n",
    "        # If t_min becomes greater than t_max, the line does not intersect the box\n",
    "        if t_min > t_max:\n",
    "            return []\n",
    "\n",
    "    # The line intersects the box. The intersection occurs between t_min and t_max.\n",
    "    # The actual points of entry and exit within the original line segment range [0, 1] are determined here.\n",
    "\n",
    "    # Clamp t_min and t_max to the line segment range [0, 1] if you're working with a segment\n",
    "    # If using an infinite line, skip this clamping step.\n",
    "    t_start = max(0, t_min)\n",
    "    t_end = min(1, t_max)\n",
    "\n",
    "    if t_start <= t_end:\n",
    "        # Calculate intersection points\n",
    "        if t_start >= 0 and t_start <= 1:\n",
    "            points = line_start + t_start * line_dir\n",
    "            intersection_points.append(points)\n",
    "        if t_end >= 0 and t_end <= 1 and t_end != t_start: # Avoid adding the same point twice\n",
    "            points = line_start + t_end * line_dir\n",
    "            intersection_points.append(points)\n",
    "            \n",
    "    return intersection_points\n",
    "\n",
    "# --- Example Usage (3D) ---\n",
    "line_start = np.array([-1.0, 0.5, 0.5])\n",
    "line_end = np.array([2.0, 0.5, 0.5])\n",
    "box_min = np.array([0.0, 0.0, 0.0])\n",
    "box_max = np.array([1.0, 1.0, 1.0])\n",
    "\n",
    "intersections = line_box_intersection(line_start, line_end, box_min, box_max)\n",
    "\n",
    "if intersections:\n",
    "    print(f\"Intersection points: {intersections}\")\n",
    "else:\n",
    "    print(\"No intersection points found.\")\n",
    "\n",
    "# --- Example Usage (2D) ---\n",
    "# For 2D, the logic remains the same, just use 2-element arrays\n",
    "line_start_2d = np.array([0.5, 0.0])\n",
    "line_end_2d = np.array([1.5, 1])\n",
    "box_min_2d = np.array([0.0, 0.0])\n",
    "box_max_2d = np.array([1.0, 1.0])\n",
    "\n",
    "intersections_2d = line_box_intersection(line_start_2d, line_end_2d, box_min_2d, box_max_2d)\n",
    "\n",
    "if intersections_2d:\n",
    "    print(f\"2D Intersection points: {intersections_2d}\")\n",
    "else:\n",
    "    print(\"No 2D intersection points found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-10.0, 20.0), (-4.0, 31.0), (2.0, 42.0), (8.0, 53.0), (14.0, 64.0), (20.0, 75.0)]\n"
     ]
    }
   ],
   "source": [
    "def split_line_segment(start_point, end_point, segments):\n",
    "    \"\"\"\n",
    "    Splits a line segment into a list of equally spaced points (including start and end).\n",
    "\n",
    "    Args:\n",
    "        start_point (tuple): (x1, y1)\n",
    "        end_point (tuple): (x2, y2)\n",
    "        segments (int): The number of line segments (points will be segments + 1).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of points as tuples [(x1, y1), ..., (x2, y2)]\n",
    "    \"\"\"\n",
    "    #thanks Google's AI\n",
    "    if segments < 1:\n",
    "        raise ValueError(\"Segments must be at least 1\")\n",
    "\n",
    "    x1, y1 = start_point\n",
    "    x2, y2 = end_point\n",
    "\n",
    "    x_delta = (x2 - x1) / float(segments)\n",
    "    y_delta = (y2 - y1) / float(segments)\n",
    "\n",
    "    points = []\n",
    "    for i in range(segments + 1):\n",
    "        points.append((x1 + i * x_delta, y1 + i * y_delta))\n",
    "    \n",
    "    return points\n",
    "\n",
    "# Example Usage\n",
    "start = (-10, 20)\n",
    "end = (20, 75)\n",
    "num_segments = 5 # This will produce 6 points (5 intermediate + start/end)\n",
    "\n",
    "result_points = split_line_segment(start, end, num_segments)\n",
    "print(result_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2D Points:\n",
      "[[ 0.   0. ]\n",
      " [ 2.5  5. ]\n",
      " [ 5.  10. ]\n",
      " [ 7.5 15. ]\n",
      " [10.  20. ]]\n",
      "--------------------\n",
      "4D Points:\n",
      "[[ 1.  2.  3.  4.]\n",
      " [ 4.  8. 12. 16.]\n",
      " [ 7. 14. 21. 28.]\n",
      " [10. 20. 30. 40.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_line_nd(start_point, end_point, num_points):\n",
    "    \"\"\"\n",
    "    Generates num_points equally spaced along a line segment between \n",
    "    start_point and end_point in N dimensions.\n",
    "\n",
    "    Args:\n",
    "        start_point (iterable): The starting point (e.g., [x1, y1, z1])\n",
    "        end_point (iterable): The ending point (e.g., [x2, y2, z2])\n",
    "        num_points (int): The total number of points to generate (must be >= 2).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of shape (num_points, N) containing the points.\n",
    "    \"\"\"\n",
    "    if num_points < 2:\n",
    "        return np.array([start_point, end_point])\n",
    "\n",
    "    # Convert to numpy arrays for easier calculation\n",
    "    p1 = np.array(start_point)\n",
    "    p2 = np.array(end_point)\n",
    "    \n",
    "    # Generate the interpolation factors (0 to 1)\n",
    "    # The number of steps is num_points - 1\n",
    "    t = np.linspace(0, 1, num_points)\n",
    "\n",
    "    # Linear interpolation formula: P(t) = P1 + t * (P2 - P1)\n",
    "    # The result will be an array of points\n",
    "    points = p1 + t[:, np.newaxis] * (p2 - p1)\n",
    "    \n",
    "    return points\n",
    "\n",
    "# Example 1: 2D Points\n",
    "start_2d = [0, 0]\n",
    "end_2d = [10, 20]\n",
    "points_2d = split_line_nd(start_2d, end_2d, 5)\n",
    "print(\"2D Points:\")\n",
    "print(points_2d)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Example 2: 4D Points\n",
    "start_4d = [1, 2, 3, 4]\n",
    "end_4d = [10, 20, 30, 40]\n",
    "points_4d = split_line_nd(start_4d, end_4d, 4)\n",
    "print(\"4D Points:\")\n",
    "print(points_4d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.         0.         0.        ]\n",
      " [0.57142857 0.07142857 0.03571429 0.03571429]\n",
      " [0.64285714 0.14285714 0.07142857 0.07142857]\n",
      " [0.71428571 0.21428571 0.10714286 0.10714286]\n",
      " [0.78571429 0.28571429 0.14285714 0.14285714]\n",
      " [0.85714286 0.35714286 0.17857143 0.17857143]\n",
      " [0.92857143 0.42857143 0.21428571 0.21428571]\n",
      " [1.         0.5        0.25       0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "def getcpoints(x0,u,num_segments=5):\n",
    "    x1=x0+u\n",
    "    box_min = np.zeros_like(x0)\n",
    "    box_max = np.ones_like(x0)\n",
    "    start,end = line_box_intersection(x0, x1, box_min, box_max)\n",
    "    #print(start,end)\n",
    "    result_points = split_line_nd(start, end, num_segments)\n",
    "    return(result_points)\n",
    "\n",
    "line_start_2d = np.array([0.5, 0.0,0,0])\n",
    "line_dir = np.array([1, 1,0.5,0.5])\n",
    "print(getcpoints(line_start_2d,line_dir,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0. ]\n",
      " [0.2 0.  0.  0. ]\n",
      " [0.4 0.  0.  0. ]\n",
      " [0.6 0.  0.  0. ]\n",
      " [0.8 0.  0.  0. ]\n",
      " [1.  0.  0.  0. ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3417441/3542054482.py:28: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t1 = (box_min[i] - line_start[i]) * inv_dir[i]\n"
     ]
    }
   ],
   "source": [
    "line_start_2d = np.array([0, 0.0,0,0])\n",
    "line_dir = np.array([1, 0,0,0])\n",
    "print(getcpoints(line_start_2d,line_dir,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0. ]\n",
      " [0.2 0.2 0.2 0.2]\n",
      " [0.4 0.4 0.4 0.4]\n",
      " [0.6 0.6 0.6 0.6]\n",
      " [0.8 0.8 0.8 0.8]\n",
      " [1.  1.  1.  1. ]]\n"
     ]
    }
   ],
   "source": [
    "line_start_2d = np.array([0, 0.0,0,0])\n",
    "line_dir = np.array([1, 1,1,1])\n",
    "print(getcpoints(line_start_2d,line_dir,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.  0.  0. ]\n",
      " [0.  0.2 0.2 0. ]\n",
      " [0.  0.4 0.4 0. ]\n",
      " [0.  0.6 0.6 0. ]\n",
      " [0.  0.8 0.8 0. ]\n",
      " [0.  1.  1.  0. ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3417441/3542054482.py:28: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t1 = (box_min[i] - line_start[i]) * inv_dir[i]\n"
     ]
    }
   ],
   "source": [
    "line_start_2d = np.array([0, 0.0,0,0])\n",
    "line_dir = np.array([0, 1,1,0])\n",
    "print(getcpoints(line_start_2d,line_dir,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.] [0.25 0.5  0.75 1.  ]\n",
      "[[0.   0.   0.   0.  ]\n",
      " [0.05 0.1  0.15 0.2 ]\n",
      " [0.1  0.2  0.3  0.4 ]\n",
      " [0.15 0.3  0.45 0.6 ]\n",
      " [0.2  0.4  0.6  0.8 ]\n",
      " [0.25 0.5  0.75 1.  ]]\n"
     ]
    }
   ],
   "source": [
    "line_start_2d = np.array([0, 0.0,0,0])\n",
    "line_dir = np.array([0.25, 0.5,0.75,1])\n",
    "print(getcpoints(line_start_2d,line_dir,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.  ]\n",
      " [0.   0.06 0.2  0.2 ]\n",
      " [0.   0.12 0.4  0.4 ]\n",
      " [0.   0.18 0.6  0.6 ]\n",
      " [0.   0.24 0.8  0.8 ]\n",
      " [0.   0.3  1.   1.  ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3417441/3542054482.py:28: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  t1 = (box_min[i] - line_start[i]) * inv_dir[i]\n"
     ]
    }
   ],
   "source": [
    "line_start_2d = np.array([0, 0.0,0,0])\n",
    "line_dir = np.array([0, 0.3,1,1])\n",
    "print(getcpoints(line_start_2d,line_dir,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.,  4.,  5.],\n",
      "        [ 6.,  0.,  7.,  8.],\n",
      "        [ 0.,  9., 10., 11.],\n",
      "        [12., 13.,  0.,  0.]])\n",
      "1 tensor([[19., 19.,  5.,  0.],\n",
      "        [ 0.,  0.,  3.,  7.],\n",
      "        [11., 11.,  0.,  1.],\n",
      "        [19., 19.,  5.,  0.]])\n",
      "2 tensor([[11., 11.,  0.,  1.],\n",
      "        [19., 19.,  5.,  0.],\n",
      "        [ 0.,  0.,  3.,  7.],\n",
      "        [19., 19.,  5.,  0.]])\n",
      "tensor([[30., 19., 11., 38.],\n",
      "        [30., 19., 11., 38.],\n",
      "        [ 5.,  8.,  3., 10.],\n",
      "        [ 1.,  7.,  8.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "import torch\n",
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "W=np.array([[3,3,4,5],\n",
    "[6,0,7,8],\n",
    "[0,9,10,11],\n",
    "[12,13,0,0]])\n",
    "\n",
    "\n",
    "Pin2=np.array([[0,0,1],\n",
    "[1,0,0],\n",
    "[0,1,0],\n",
    "[1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "C=np.array([[0,19,5],\n",
    "[7,0,3],\n",
    "[1,11,0]])\n",
    "\n",
    "Pout=np.array([[0,1,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "E=torch.from_numpy(E).float()\n",
    "W=torch.from_numpy(W).float()\n",
    "Pin=torch.from_numpy(Pin).float()\n",
    "Pin2=torch.from_numpy(Pin2).float()\n",
    "Pout=torch.from_numpy(Pout).float()\n",
    "C=torch.from_numpy(C).float()\n",
    "\n",
    "\n",
    "print(W)\n",
    "parents=[1,2]\n",
    "layer_name=3\n",
    "P={}\n",
    "P[1]=Pin\n",
    "P[2]=Pin2\n",
    "P[3]=Pout\n",
    "comm_costs=0\n",
    "for parent in parents:\n",
    "    #print(ra.P[parent].shape)\n",
    "    #print(ra.C.shape)\n",
    "    #print(torch.transpose(ra.P[layer_name],0,1).shape )\n",
    "    #print(ADMM.P[parent].device,C.device,ADMM.P[layer_name].device)\n",
    "    cc=P[parent] @ C @ torch.transpose(P[layer_name],0,1)   \n",
    "    print(parent,cc)\n",
    "    comm_costs+=cc\n",
    "#print(comm_costs.shape)\n",
    "print( torch.transpose(comm_costs,0,1))\n",
    "parents = [self.adoptparents(P) for P in self.partition[name].get('parents', [])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([2])\n",
    "w=np.array([3])\n",
    "y=x**2\n",
    "y+=1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 2: 2, 'd2': {4: 'ha!', 3: 3}}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = {1: 1, 2: 2}\n",
    "d2 = {4: 'ha!', 3: 3}\n",
    "d1['d2'] = d2\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "t=torch.tensor([[2,2,4],[1,1,9]])\n",
    "t=t.unsqueeze(2)\n",
    "t=t.unsqueeze(3)\n",
    "#t=t.repeat((1,1,3,3))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=torch.tensor([[1,0,0],[0,1,0],[0,0,1],[0,1,0],[0,0,1],[0,0,1],[1,0,0],[0,1,0],[1,0,0],[0,1,0],[0,0,1],[0,1,0],[0,0,1],[0,0,1],[1,0,0],[0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 1, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 0, 0],\n",
       "        [0, 1, 0]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 6, 8, 14]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=t[:,0]==1\n",
    "i=x.nonzero().squeeze().tolist()\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 6, 8, 14]\n",
      "[1, 3, 7, 9, 11, 15]\n",
      "[2, 4, 5, 10, 12, 13]\n",
      "[[0, 6, 8, 14], [1, 3, 7, 9, 11, 15], [2, 4, 5, 10, 12, 13]]\n"
     ]
    }
   ],
   "source": [
    "m=t.shape[1]\n",
    "L=[]\n",
    "i=t.nonzero()\n",
    "for j in range(m):\n",
    "    l=i[i[:,1]==j][:,0].tolist()\n",
    "    print(l)\n",
    "    L.append(l)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 0 0]]\n",
      "[[ 6.55743852  5.          5.        ]\n",
      " [10.          7.          9.21954446]\n",
      " [14.2126704  13.45362405 10.        ]\n",
      " [17.69180601 13.         12.        ]]\n",
      "[ 6.55743852 10.         14.2126704  17.69180601]\n",
      "12 6.0 4 2.0 8 0.16666666666666666\n",
      "c2 [[1 0 1]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [0 1 1]]\n",
      "res1 [[ 6.55743852  0.          5.        ]\n",
      " [10.          0.          9.21954446]\n",
      " [14.2126704  13.45362405  0.        ]\n",
      " [ 0.         13.         12.        ]]\n",
      "13.07560400784562\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 0. 0.]]\n",
      "[[0. 1. 1. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "W=np.array([[3,3,4,5],\n",
    "[6,0,7,8],\n",
    "[0,9,10,11],\n",
    "[12,13,0,0]])\n",
    "\n",
    "\n",
    "Pin2=np.array([[0,0,1],\n",
    "[1,0,0],\n",
    "[0,1,0],\n",
    "[1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "C=np.array([[0,19,5],\n",
    "[7,0,3],\n",
    "[1,11,0]])\n",
    "\n",
    "Pout=np.array([[0,1,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "#all parents\n",
    "P3=Pin+Pin2\n",
    "P3[P3!=0]=1\n",
    "print(P3)\n",
    "\n",
    "pr=1/2\n",
    "\n",
    "\n",
    "E=W**2\n",
    "needs=E@P3\n",
    "needs=np.sqrt((needs))\n",
    "print(needs)\n",
    "row_l2_norm = LA.norm(W[:,[0,1,3]], axis=1)\n",
    "print(row_l2_norm)\n",
    "\n",
    "total=needs.shape[0]*needs.shape[1]\n",
    "keep=(1-pr)*total\n",
    "aligned_keep=needs.shape[0]\n",
    "misaligned_keep=keep-aligned_keep\n",
    "misaligned_total=total-aligned_keep\n",
    "apr=misaligned_keep/total\n",
    "print(total,keep,aligned_keep,misaligned_keep,misaligned_total,apr)\n",
    "\n",
    "c2=1-Pout\n",
    "print(\"c2\", c2)\n",
    "res1=needs*c2\n",
    "print(\"res1\", res1)\n",
    "t=torch.quantile(torch.from_numpy(res1), 1-apr).item()\n",
    "print(t)\n",
    "res1[res1>t]=1\n",
    "res1[res1!=1]=0\n",
    "print(res1)\n",
    "res1=res1+Pout\n",
    "print(res1)\n",
    "mask=res1@np.transpose(P3)\n",
    "mask[mask!=0]=1\n",
    "print(mask)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1.],\n",
      "        [1., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor([[ 3.,  3.,  4.,  5.],\n",
      "        [ 6.,  0.,  7.,  8.],\n",
      "        [ 0.,  9., 10., 11.],\n",
      "        [12., 13.,  0.,  0.]])\n",
      "tensor([[11.,  7.,  7.],\n",
      "        [14.,  7., 13.],\n",
      "        [20., 19., 10.],\n",
      "        [25., 13., 12.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor([[11.,  0.,  7.],\n",
      "        [14.,  0., 13.],\n",
      "        [20., 19.,  0.],\n",
      "        [ 0., 13., 12.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor([[0., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 0., 1.]])\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "W=np.array([[3,3,4,5],\n",
    "[6,0,7,8],\n",
    "[0,9,10,11],\n",
    "[12,13,0,0]])\n",
    "\n",
    "\n",
    "Pin2=np.array([[0,0,1],\n",
    "[1,0,0],\n",
    "[0,1,0],\n",
    "[1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "C=np.array([[0,19,5],\n",
    "[7,0,3],\n",
    "[1,11,0]])\n",
    "\n",
    "Pout=np.array([[0,1,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "E=torch.from_numpy(E).float()\n",
    "W=torch.from_numpy(W).float()\n",
    "Pin=torch.from_numpy(Pin).float()\n",
    "Pin2=torch.from_numpy(Pin2).float()\n",
    "Pout=torch.from_numpy(Pout).float()\n",
    "C=torch.from_numpy(C).float()\n",
    "\n",
    "P=Pin+Pin2\n",
    "P[P!=0]=1\n",
    "\n",
    "prune_ratio=0.5\n",
    "print(P)\n",
    "print(W)\n",
    "E=W**2\n",
    "\n",
    "needs=torch.sqrt(E@P)\n",
    "#W[W!=0]=1\n",
    "needs=W@P\n",
    "print(needs)\n",
    "\n",
    "\n",
    "#adjust semantics pr is ratio of pruned to total\n",
    "total=needs.shape[0]*needs.shape[1]\n",
    "aligned_keep=needs.shape[0]\n",
    "misaligned_total=total-aligned_keep\n",
    "\n",
    "misaligned_keep=(1-prune_ratio)*misaligned_total            \n",
    "apr=misaligned_keep/total\n",
    "\n",
    "c2=1-Pout\n",
    "res1=needs*c2\n",
    "print(Pout)\n",
    "print(res1)\n",
    "\n",
    "t=torch.quantile(res1, 1-apr) \n",
    "res1[res1>t]=1\n",
    "res1[res1!=1]=0\n",
    "res1=res1+Pout\n",
    "print(res1)\n",
    "\n",
    "mask=res1@torch.transpose(P,0,1)\n",
    "mask[mask!=0]=1\n",
    "print(mask)\n",
    "\n",
    "\n",
    "res2=1-res1\n",
    "print(res2)\n",
    "\n",
    "mask2=res2@torch.transpose(P,0,1)\n",
    "mask2[mask2!=0]=1\n",
    "mask2=1-mask2\n",
    "print(mask2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1.],\n",
      "        [1., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor([[ 3.,  3.,  4.,  5.],\n",
      "        [ 6.,  0.,  7.,  8.],\n",
      "        [ 0.,  9., 10., 11.],\n",
      "        [12., 13.,  0.,  0.]])\n",
      "tensor([[11.,  7.,  7.],\n",
      "        [14.,  7., 13.],\n",
      "        [20., 19., 10.],\n",
      "        [25., 13., 12.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor([[11.,  0.,  7.],\n",
      "        [14.,  0., 13.],\n",
      "        [20., 19.,  0.],\n",
      "        [ 0., 13., 12.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 0., 0.]])\n",
      "tensor([[0., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 0., 1.]])\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1.]])\n",
      "!!!!!!!\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor(12.)\n",
      "kernels 16 tensor(12.) tensor(0.7500)\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "rows 12 tensor(8.) tensor(0.6667)\n",
      "off rows 8 tensor(5.) tensor(0.6250)\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "W=np.array([[3,3,4,5],\n",
    "[6,0,7,8],\n",
    "[0,9,10,11],\n",
    "[12,13,0,0]])\n",
    "\n",
    "\n",
    "Pin2=np.array([[0,0,1],\n",
    "[1,0,0],\n",
    "[0,1,0],\n",
    "[1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "C=np.array([[0,19,5],\n",
    "[7,0,3],\n",
    "[1,11,0]])\n",
    "\n",
    "Pout=np.array([[0,1,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "E=torch.from_numpy(E).float()\n",
    "W=torch.from_numpy(W).float()\n",
    "Pin=torch.from_numpy(Pin).float()\n",
    "Pin2=torch.from_numpy(Pin2).float()\n",
    "Pout=torch.from_numpy(Pout).float()\n",
    "C=torch.from_numpy(C).float()\n",
    "\n",
    "P=Pin+Pin2\n",
    "P[P!=0]=1\n",
    "\n",
    "prune_ratio=0.5\n",
    "print(P)\n",
    "print(W)\n",
    "E=W**2\n",
    "\n",
    "needs=torch.sqrt(E@P)\n",
    "#W[W!=0]=1\n",
    "needs=W@P\n",
    "print(needs)\n",
    "\n",
    "\n",
    "#adjust semantics pr is ratio of pruned to total\n",
    "total=needs.shape[0]*needs.shape[1]\n",
    "aligned_keep=needs.shape[0]\n",
    "misaligned_total=total-aligned_keep\n",
    "\n",
    "misaligned_keep=(1-prune_ratio)*misaligned_total            \n",
    "apr=misaligned_keep/total\n",
    "\n",
    "c2=1-Pout\n",
    "res1=needs*c2\n",
    "print(Pout)\n",
    "print(res1)\n",
    "\n",
    "t=torch.quantile(res1, 1-apr) \n",
    "res1[res1>t]=1\n",
    "res1[res1!=1]=0\n",
    "res1=res1+Pout\n",
    "print(res1)\n",
    "\n",
    "mask=res1@torch.transpose(P,0,1)\n",
    "mask[mask!=0]=1\n",
    "print(mask)\n",
    "\n",
    "\n",
    "res2=1-res1\n",
    "print(res2)\n",
    "\n",
    "mask2=res2@torch.transpose(P,0,1)\n",
    "mask2[mask2!=0]=1\n",
    "mask2=1-mask2\n",
    "print(mask2)\n",
    "\n",
    "\n",
    "print(\"!!!!!!!\")\n",
    "W2=W*mask2\n",
    "zeros=0\n",
    "total=0\n",
    "zerosp=0\n",
    "totalp=0\n",
    "E=W2\n",
    "E[E!=0]=1\n",
    "print(E)\n",
    "(n1,n2)=E.shape\n",
    "total=n1*n2\n",
    "zeros=total-E.sum()\n",
    "print(zeros)\n",
    "print(\"kernels\",total,zeros,zeros/total)\n",
    "#lin alg\n",
    "needs=E@P\n",
    "needs[needs!=0]=1\n",
    "print(needs)\n",
    "(n,m)=needs.shape\n",
    "totalp=n*m\n",
    "zerosp=totalp-needs.sum()\n",
    "print(\"rows\",totalp,zerosp,zerosp/totalp)\n",
    "\n",
    "c2=1-Pout\n",
    "offneeds=needs*c2\n",
    "offtotalp=totalp-n\n",
    "offzerosp=offtotalp-offneeds.sum()\n",
    "print(\"off rows\",offtotalp,offzerosp,offzerosp/offtotalp)\n",
    "print(prune_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  3.,  4.,  5.],\n",
      "        [ 6.,  0.,  7.,  8.],\n",
      "        [ 0.,  9., 10., 11.],\n",
      "        [12., 13.,  0.,  0.]])\n",
      "1 tensor([[19., 19.,  5.,  0.],\n",
      "        [ 0.,  0.,  3.,  7.],\n",
      "        [11., 11.,  0.,  1.],\n",
      "        [19., 19.,  5.,  0.]])\n",
      "2 tensor([[11., 11.,  0.,  1.],\n",
      "        [19., 19.,  5.,  0.],\n",
      "        [ 0.,  0.,  3.,  7.],\n",
      "        [19., 19.,  5.,  0.]])\n",
      "tensor([[30., 19., 11., 38.],\n",
      "        [30., 19., 11., 38.],\n",
      "        [ 5.,  8.,  3., 10.],\n",
      "        [ 1.,  7.,  8.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "#test 2\n",
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "W=np.array([[3,3,4,5],\n",
    "[6,0,7,8],\n",
    "[0,9,10,11],\n",
    "[12,13,0,0]])\n",
    "\n",
    "\n",
    "Pin2=np.array([[0,0,1],\n",
    "[1,0,0],\n",
    "[0,1,0],\n",
    "[1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "C=np.array([[0,19,5],\n",
    "[7,0,3],\n",
    "[1,11,0]])\n",
    "\n",
    "Pout=np.array([[0,1,0],\n",
    "[0,1,0],\n",
    "[0,0,1],\n",
    "[1,0,0]])\n",
    "\n",
    "E=torch.from_numpy(E).float()\n",
    "W=torch.from_numpy(W).float()\n",
    "Pin=torch.from_numpy(Pin).float()\n",
    "Pin2=torch.from_numpy(Pin2).float()\n",
    "Pout=torch.from_numpy(Pout).float()\n",
    "C=torch.from_numpy(C).float()\n",
    "\n",
    "\n",
    "print(W)\n",
    "parents=[1,2]\n",
    "layer_name=3\n",
    "P={}\n",
    "P[1]=Pin\n",
    "P[2]=Pin2\n",
    "P[3]=Pout\n",
    "comm_costs=0\n",
    "for parent in parents:\n",
    "    #print(ra.P[parent].shape)\n",
    "    #print(ra.C.shape)\n",
    "    #print(torch.transpose(ra.P[layer_name],0,1).shape )\n",
    "    #print(ADMM.P[parent].device,C.device,ADMM.P[layer_name].device)\n",
    "    cc=P[parent] @ C @ torch.transpose(P[layer_name],0,1)   \n",
    "    print(parent,cc)\n",
    "    comm_costs+=cc\n",
    "#print(comm_costs.shape)\n",
    "print( torch.transpose(comm_costs,0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [1 1]]\n",
      "[[1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[[ 7  0]\n",
      " [ 7  0]\n",
      " [ 0 19]\n",
      " [ 0 19]]\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0],\n",
    "[0,1],\n",
    "[1,0],\n",
    "[1,0]])\n",
    "\n",
    "C=np.array([[0,19],\n",
    "[7,0]])\n",
    "\n",
    "Pout=np.array([[0,1],\n",
    "[0,1],\n",
    "[1,0],\n",
    "[1,0]])\n",
    "\n",
    "\n",
    "needs=E@Pin\n",
    "print(needs)\n",
    "needs[needs!=0]=1\n",
    "a=1\n",
    "#needs=torch.tanh(torch.from_numpy(a*needs)).numpy()\n",
    "#needs=pnormalize(needs)\n",
    "print(needs)\n",
    "costs=Pout@C\n",
    "print(costs)\n",
    "\n",
    "res2=needs*costs\n",
    "print(np.sum(res2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 1]\n",
      " [3 0]\n",
      " [2 1]\n",
      " [1 1]]\n",
      "[[1 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]]\n",
      "[[ 0 19]\n",
      " [ 7  0]]\n",
      "[[ 7 19]\n",
      " [ 0 19]\n",
      " [ 7 19]\n",
      " [ 7 19]]\n"
     ]
    }
   ],
   "source": [
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "Pin=np.array([[1,0],\n",
    "[0,1],\n",
    "[1,0],\n",
    "[1,0]])\n",
    "\n",
    "C=np.array([[0,19],\n",
    "[7,0]])\n",
    "\n",
    "Pout=np.array([[0,1],\n",
    "[0,1],\n",
    "[1,0],\n",
    "[1,0]])\n",
    "\n",
    "\n",
    "needs=E@Pin\n",
    "print(needs)\n",
    "needs[needs!=0]=1\n",
    "a=1\n",
    "#needs=torch.tanh(torch.from_numpy(a*needs)).numpy()\n",
    "#needs=pnormalize(needs)\n",
    "print(needs)\n",
    "costs=C\n",
    "print(costs)\n",
    "\n",
    "res2=needs@costs\n",
    "print(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def ppp(f):\n",
    "    # Create an array of x values\n",
    "    x = np.linspace(0, 10, 1000)\n",
    "    a=20\n",
    "    # Calculate the PDF for each x value\n",
    "    pdf = torch.tanh(torch.from_numpy(a*x)).numpy()\n",
    "\n",
    "    # Plot the PDF\n",
    "    plt.plot(x, pdf)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f=torch.tanh\n",
    "ppp(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes the softmax function for a given numpy array.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Softmax output.\n",
    "    \"\"\"\n",
    "    e_x = np.exp((x - np.max(x))) # Subtracting np.max(x) for numerical stability\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pnormalize(x,p=2):\n",
    "    \"\"\"\n",
    "    Computes the softmax function for a given numpy array.\n",
    "\n",
    "    Args:\n",
    "        x (numpy.ndarray): Input array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Softmax output.\n",
    "    \"\"\"\n",
    "    e_x = x**p # Subtracting np.max(x) for numerical stability\n",
    "    return e_x #/ e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.6 1.4]\n",
      " [2.4 0.6]\n",
      " [1.8 1.2]\n",
      " [1.  1. ]]\n",
      "[[6.76 1.96]\n",
      " [5.76 0.36]\n",
      " [3.24 1.44]\n",
      " [1.   1.  ]]\n",
      "[[ 5.6  3.8]\n",
      " [ 5.6  3.8]\n",
      " [ 1.4 15.2]\n",
      " [ 1.4 15.2]]\n",
      "[45.304 33.624 26.424 16.6  ]\n"
     ]
    }
   ],
   "source": [
    "E=np.array([[1,1,1,1],\n",
    "[1,0,1,1],\n",
    "[0,1,1,1],\n",
    "[1,1,0,0]])\n",
    "\n",
    "Pin=np.array([[0.8,0.2],\n",
    "[0.2,0.8],\n",
    "[0.8,0.2],\n",
    "[0.8,0.2]])\n",
    "\n",
    "C=np.array([[0,19],\n",
    "[7,0]])\n",
    "\n",
    "Pout=np.array([[0.2,0.8],\n",
    "[0.2,0.8],\n",
    "[0.8,0.2],\n",
    "[0.8,0.2]])\n",
    "\n",
    "\n",
    "needs=E@Pin\n",
    "print(needs)\n",
    "#needs[needs!=0]=1\n",
    "needs=pnormalize(needs)\n",
    "\n",
    "print(needs)\n",
    "costs=Pout@C\n",
    "print(costs)\n",
    "\n",
    "res2=needs*costs\n",
    "print(np.sum(res2,axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create topolgies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import topologies\n",
    "import networkx as nx\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_maps_from_E(N,E):\n",
    "    #maps=np.full((N,N),10e10) #  99\n",
    "    maps=np.full((N,N),99) #  99\n",
    "    for i in range(N):\n",
    "        maps[i,i]=0\n",
    "    for (i,j,v) in E:\n",
    "        maps[i,j]=v\n",
    "        maps[j,i]=v\n",
    "    return maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yaml_file(N, model_type, maps,costs=False):\n",
    "    # valid: resnet18, resnet101\n",
    "    if model_type == 'resnet18':\n",
    "        layers_num = 20\n",
    "        layers_str = \"[inputs, conv1.weight, layer1.0.conv1.weight, layer1.0.conv2.weight, layer1.1.conv1.weight, layer1.1.conv2.weight, layer2.0.conv1.weight, layer2.0.conv2.weight, layer2.0.shortcut.0.weight, layer2.1.conv1.weight, layer2.1.conv2.weight, layer3.0.conv1.weight, layer3.0.conv2.weight, layer3.0.shortcut.0.weight, layer3.1.conv1.weight, layer3.1.conv2.weight, layer4.0.conv1.weight, layer4.0.conv2.weight, layer4.0.shortcut.0.weight, layer4.1.conv1.weight, layer4.1.conv2.weight]\"\n",
    "\n",
    "    if model_type == 'resnet101':\n",
    "        layers_num = 70\n",
    "        layers_str =  \"[inputs, conv1.weight, layer1.0.conv1.weight, layer1.0.conv2.weight, layer1.1.conv1.weight, \\\n",
    "            layer1.1.conv2.weight, layer1.2.conv1.weight, layer1.2.conv2.weight, layer2.0.conv1.weight, \\\n",
    "            layer2.0.conv2.weight, layer2.0.shortcut.0.weight, layer2.1.conv1.weight, layer2.1.conv2.weight, \\\n",
    "            layer2.2.conv1.weight, layer2.2.conv2.weight, layer2.3.conv1.weight, layer2.3.conv2.weight, \\\n",
    "            layer3.0.conv1.weight, layer3.0.conv2.weight, layer3.0.shortcut.0.weight, layer3.1.conv1.weight, \\\n",
    "            layer3.1.conv2.weight, layer3.2.conv1.weight, layer3.2.conv2.weight, layer3.3.conv1.weight, \\\n",
    "            layer3.3.conv2.weight, layer3.4.conv1.weight, layer3.4.conv2.weight, layer3.5.conv1.weight, \\\n",
    "            layer3.5.conv2.weight, layer3.6.conv1.weight, layer3.6.conv2.weight, layer3.7.conv1.weight, \\\n",
    "            layer3.7.conv2.weight, layer3.8.conv1.weight, layer3.8.conv2.weight, layer3.9.conv1.weight, \\\n",
    "            layer3.9.conv2.weight, layer3.10.conv1.weight, layer3.10.conv2.weight, layer3.11.conv1.weight, \\\n",
    "            layer3.11.conv2.weight, layer3.12.conv1.weight, layer3.12.conv2.weight, layer3.13.conv1.weight, \\\n",
    "            layer3.13.conv2.weight, layer3.14.conv1.weight, layer3.14.conv2.weight, layer3.15.conv1.weight, \\\n",
    "            layer3.15.conv2.weight, layer3.16.conv1.weight, layer3.16.conv2.weight, layer3.17.conv1.weight, \\\n",
    "            layer3.17.conv2.weight, layer3.18.conv1.weight, layer3.18.conv2.weight, layer3.19.conv1.weight, \\\n",
    "            layer3.19.conv2.weight, layer3.20.conv1.weight, layer3.20.conv2.weight, layer3.21.conv1.weight, \\\n",
    "            layer3.21.conv2.weight, layer3.22.conv1.weight, layer3.22.conv2.weight, layer4.0.conv1.weight, \\\n",
    "            layer4.0.conv2.weight, layer4.0.shortcut.0.weight, layer4.1.conv1.weight, layer4.1.conv2.weight, \\\n",
    "            layer4.2.conv1.weight, layer4.2.conv2.weight]\"  \n",
    "\n",
    "    yaml_dict = {}\n",
    "    yaml_dict['bn_partitions'] = [N] * 9\n",
    "    initial_budget = [1] * N\n",
    "    yaml_dict['budgets'] = np.tile(initial_budget, (layers_num, 1)).tolist()\n",
    "    yaml_dict['initial_budget'] = initial_budget\n",
    "    yaml_dict['input_partition'] = initial_budget\n",
    "    yaml_dict['layers'] = layers_str\n",
    "    yaml_dict['maps'] = maps.tolist() #np.array2string(maps, threshold=10000)\n",
    "    yaml_dict['num'] = N\n",
    "    yaml_str = json.dumps(yaml_dict)\n",
    "\n",
    "    yaml_str = yaml_str.replace(\"\\\"\", \"\")\n",
    "    yaml_str = yaml_str.replace(\", budgets: \", \"\\nbudgets: \")\n",
    "    yaml_str = yaml_str.replace(\", initial_budget: \", \"\\ninitial_budget: \")\n",
    "    yaml_str = yaml_str.replace(\", input_partition: \", \"\\ninput_partition: \")\n",
    "    yaml_str = yaml_str.replace(\", layers: \", \"\\nlayers: \")\n",
    "    yaml_str = yaml_str.replace(\", maps: \", \"\\nmaps: \")\n",
    "    yaml_str = yaml_str.replace(\", num: \", \"\\nnum: \")\n",
    "    yaml_str = yaml_str.replace(\"{\", \"\")\n",
    "    yaml_str = yaml_str.replace(\"}\", \"\")\n",
    "    '''\n",
    "    yaml_str = yaml_str.replace(\", '\", \"\\n'\")\n",
    "    yaml_str = yaml_str.replace(\"{\", \"\")\n",
    "    yaml_str = yaml_str.replace(\"}\", \"\")\n",
    "    yaml_str = yaml_str.replace(\"array(\", \"\")\n",
    "    yaml_str = yaml_str.replace(\"]])\", \"]]\")\n",
    "    yaml_str = yaml_str.replace(\"],\", \"]\\n\")\n",
    "    '''\n",
    "    yaml_str = yaml_str.replace(\"layer1.1.conv1.weight,\", \"layer1.1.conv1.weight,\\n\")\n",
    "    yaml_str = yaml_str.replace(\"layer2.0.shortcut.0.weight,\", \"layer2.0.shortcut.0.weight,\\n\")\n",
    "    yaml_str = yaml_str.replace(\"layer3.0.conv2.weight,\", \"layer3.0.conv2.weight,\\n\")\n",
    "    yaml_str = yaml_str.replace(\"layer4.0.conv1.weight,\", \"layer4.0.conv1.weight,\\n\")\n",
    "\n",
    "    yaml_str = yaml_str.replace(\"layer1.1.conv2.weight,\", \"        layer1.1.conv2.weight,\")\n",
    "    yaml_str = yaml_str.replace(\"layer2.1.conv1.weight,\", \"        layer2.1.conv1.weight,\")\n",
    "    yaml_str = yaml_str.replace(\"layer3.0.shortcut.0.weight,\", \"        layer3.0.shortcut.0.weight,\")\n",
    "    yaml_str = yaml_str.replace(\"layer4.0.conv2.weight\", \"        layer4.0.conv2.weight\")\n",
    "    '''\n",
    "    yaml_str = yaml_str.replace(\"\\\"\", \"\")\n",
    "    yaml_str = yaml_str.replace(\", maps: \", \"\\nmaps: \")\n",
    "    '''\n",
    "    if costs:\n",
    "        yaml_filename = \"./config/\" + model_type + \"-np\" + str(N) + \"-\" + experiment_name + \"_cost.yaml\"\n",
    "    else:\n",
    "        yaml_filename = \"./config/\" + model_type + \"-np\" + str(N) + \"-\" + experiment_name + \".yaml\"\n",
    "    print(yaml_filename)\n",
    "    with open(yaml_filename, 'w') as file:\n",
    "        file.write(yaml_str)\n",
    "    print(yaml_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs=False\n",
    "model_type = 'resnet101' # valid: resnet18, resnet101\n",
    "experiment_name = 'Dtelekom' # valid: GEANT, Dtelekom, Abilene, ServiceNetwork, WattsStrogatz, BarabasiAlbert\n",
    "\n",
    "if experiment_name == 'GEANT':\n",
    "    graph = topologies.GEANT()\n",
    "if experiment_name == 'Dtelekom':\n",
    "    graph = topologies.Dtelekom()\n",
    "if experiment_name == 'Abilene':\n",
    "    graph = topologies.Abilene()\n",
    "if experiment_name == 'ServiceNetwork':\n",
    "    graph = topologies.ServiceNetwork()\n",
    "if experiment_name == 'WattsStrogatz':\n",
    "    graph = nx.watts_strogatz_graph(n=10, k=4, p=0.5)\n",
    "if experiment_name == 'BarabasiAlbert':\n",
    "    graph = nx.barabasi_albert_graph(n=10, m=4)\n",
    "if experiment_name == 'Uniform':\n",
    "    graph = nx.complete_graph(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "273\n",
      "['Node00', 'Node01', 'Node02', 'Node03', 'Node04', 'Node05', 'Node06', 'Node07', 'Node08', 'Node09', 'Node10', 'Node11', 'Node12', 'Node13', 'Node14', 'Node15', 'Node16', 'Node17', 'Node18', 'Node19', 'Node20', 'Node21', 'Node22', 'Node23', 'Node24', 'Node25', 'Node26', 'Node27', 'Node28', 'Node29', 'Node30', 'Node31', 'Node32', 'Node33', 'Node34', 'Node35', 'Node36', 'Node37', 'Node38', 'Node39', 'Node40', 'Node41', 'Node42', 'Node43', 'Node44', 'Node45', 'Node46', 'Node47', 'Node48', 'Node49', 'Node50', 'Node51', 'Node52', 'Node53', 'Node54', 'Node55', 'Node56', 'Node57', 'Node58', 'Node59', 'Node60', 'Node61', 'Node62', 'Node63', 'Node64', 'Node65', 'Node66', 'Node67']\n",
      "[('Node00', 'Node01'), ('Node00', 'Node02'), ('Node00', 'Node03'), ('Node00', 'Node04'), ('Node00', 'Node05'), ('Node00', 'Node06'), ('Node01', 'Node58'), ('Node01', 'Node56'), ('Node01', 'Node55'), ('Node01', 'Node54'), ('Node01', 'Node52'), ('Node01', 'Node50'), ('Node01', 'Node49'), ('Node01', 'Node48'), ('Node01', 'Node47'), ('Node01', 'Node45'), ('Node01', 'Node44'), ('Node01', 'Node43'), ('Node01', 'Node42'), ('Node01', 'Node41'), ('Node01', 'Node40'), ('Node01', 'Node39'), ('Node01', 'Node38'), ('Node01', 'Node36'), ('Node01', 'Node35'), ('Node01', 'Node34'), ('Node01', 'Node32'), ('Node01', 'Node30'), ('Node01', 'Node29'), ('Node01', 'Node28'), ('Node01', 'Node25'), ('Node01', 'Node24'), ('Node01', 'Node21'), ('Node01', 'Node20'), ('Node01', 'Node18'), ('Node01', 'Node17'), ('Node01', 'Node16'), ('Node01', 'Node14'), ('Node01', 'Node11'), ('Node01', 'Node10'), ('Node01', 'Node09'), ('Node01', 'Node07'), ('Node01', 'Node06'), ('Node01', 'Node05'), ('Node01', 'Node04'), ('Node01', 'Node03'), ('Node01', 'Node02'), ('Node02', 'Node57'), ('Node02', 'Node50'), ('Node02', 'Node44'), ('Node02', 'Node32'), ('Node02', 'Node26'), ('Node02', 'Node20'), ('Node02', 'Node13'), ('Node02', 'Node11'), ('Node02', 'Node05'), ('Node02', 'Node04'), ('Node02', 'Node03'), ('Node03', 'Node60'), ('Node03', 'Node58'), ('Node03', 'Node56'), ('Node03', 'Node55'), ('Node03', 'Node54'), ('Node03', 'Node52'), ('Node03', 'Node50'), ('Node03', 'Node49'), ('Node03', 'Node48'), ('Node03', 'Node45'), ('Node03', 'Node44'), ('Node03', 'Node42'), ('Node03', 'Node41'), ('Node03', 'Node40'), ('Node03', 'Node36'), ('Node03', 'Node35'), ('Node03', 'Node33'), ('Node03', 'Node32'), ('Node03', 'Node31'), ('Node03', 'Node30'), ('Node03', 'Node29'), ('Node03', 'Node28'), ('Node03', 'Node27'), ('Node03', 'Node24'), ('Node03', 'Node21'), ('Node03', 'Node20'), ('Node03', 'Node18'), ('Node03', 'Node16'), ('Node03', 'Node14'), ('Node03', 'Node13'), ('Node03', 'Node10'), ('Node03', 'Node09'), ('Node03', 'Node08'), ('Node03', 'Node07'), ('Node03', 'Node06'), ('Node03', 'Node05'), ('Node03', 'Node04'), ('Node04', 'Node67'), ('Node04', 'Node66'), ('Node04', 'Node65'), ('Node04', 'Node64'), ('Node04', 'Node61'), ('Node04', 'Node60'), ('Node04', 'Node58'), ('Node04', 'Node57'), ('Node04', 'Node56'), ('Node04', 'Node54'), ('Node04', 'Node52'), ('Node04', 'Node50'), ('Node04', 'Node49'), ('Node04', 'Node48'), ('Node04', 'Node45'), ('Node04', 'Node44'), ('Node04', 'Node43'), ('Node04', 'Node42'), ('Node04', 'Node41'), ('Node04', 'Node40'), ('Node04', 'Node38'), ('Node04', 'Node35'), ('Node04', 'Node34'), ('Node04', 'Node32'), ('Node04', 'Node31'), ('Node04', 'Node30'), ('Node04', 'Node29'), ('Node04', 'Node28'), ('Node04', 'Node24'), ('Node04', 'Node21'), ('Node04', 'Node20'), ('Node04', 'Node18'), ('Node04', 'Node16'), ('Node04', 'Node15'), ('Node04', 'Node14'), ('Node04', 'Node13'), ('Node04', 'Node12'), ('Node04', 'Node11'), ('Node04', 'Node10'), ('Node04', 'Node09'), ('Node04', 'Node08'), ('Node04', 'Node07'), ('Node04', 'Node06'), ('Node04', 'Node05'), ('Node05', 'Node66'), ('Node05', 'Node61'), ('Node05', 'Node60'), ('Node05', 'Node58'), ('Node05', 'Node57'), ('Node05', 'Node56'), ('Node05', 'Node55'), ('Node05', 'Node54'), ('Node05', 'Node53'), ('Node05', 'Node52'), ('Node05', 'Node51'), ('Node05', 'Node50'), ('Node05', 'Node49'), ('Node05', 'Node48'), ('Node05', 'Node46'), ('Node05', 'Node43'), ('Node05', 'Node42'), ('Node05', 'Node41'), ('Node05', 'Node38'), ('Node05', 'Node36'), ('Node05', 'Node35'), ('Node05', 'Node34'), ('Node05', 'Node32'), ('Node05', 'Node30'), ('Node05', 'Node29'), ('Node05', 'Node28'), ('Node05', 'Node27'), ('Node05', 'Node26'), ('Node05', 'Node25'), ('Node05', 'Node24'), ('Node05', 'Node23'), ('Node05', 'Node22'), ('Node05', 'Node21'), ('Node05', 'Node20'), ('Node05', 'Node19'), ('Node05', 'Node16'), ('Node05', 'Node15'), ('Node05', 'Node12'), ('Node05', 'Node11'), ('Node05', 'Node09'), ('Node05', 'Node07'), ('Node05', 'Node06'), ('Node06', 'Node56'), ('Node06', 'Node55'), ('Node06', 'Node54'), ('Node06', 'Node47'), ('Node06', 'Node44'), ('Node06', 'Node43'), ('Node06', 'Node39'), ('Node06', 'Node38'), ('Node06', 'Node36'), ('Node06', 'Node32'), ('Node06', 'Node30'), ('Node06', 'Node29'), ('Node06', 'Node28'), ('Node06', 'Node26'), ('Node06', 'Node25'), ('Node06', 'Node24'), ('Node06', 'Node23'), ('Node06', 'Node22'), ('Node06', 'Node21'), ('Node06', 'Node20'), ('Node06', 'Node19'), ('Node06', 'Node15'), ('Node06', 'Node13'), ('Node06', 'Node11'), ('Node06', 'Node10'), ('Node06', 'Node09'), ('Node06', 'Node08'), ('Node06', 'Node07'), ('Node08', 'Node55'), ('Node08', 'Node40'), ('Node09', 'Node59'), ('Node09', 'Node56'), ('Node09', 'Node55'), ('Node09', 'Node53'), ('Node09', 'Node52'), ('Node09', 'Node40'), ('Node09', 'Node33'), ('Node09', 'Node15'), ('Node10', 'Node43'), ('Node11', 'Node15'), ('Node12', 'Node26'), ('Node15', 'Node33'), ('Node17', 'Node60'), ('Node17', 'Node39'), ('Node17', 'Node19'), ('Node18', 'Node33'), ('Node19', 'Node39'), ('Node23', 'Node64'), ('Node23', 'Node62'), ('Node23', 'Node61'), ('Node23', 'Node59'), ('Node23', 'Node56'), ('Node23', 'Node53'), ('Node23', 'Node51'), ('Node23', 'Node49'), ('Node23', 'Node40'), ('Node23', 'Node38'), ('Node23', 'Node37'), ('Node23', 'Node34'), ('Node23', 'Node33'), ('Node23', 'Node24'), ('Node26', 'Node65'), ('Node26', 'Node43'), ('Node26', 'Node34'), ('Node26', 'Node29'), ('Node27', 'Node59'), ('Node27', 'Node51'), ('Node28', 'Node62'), ('Node28', 'Node59'), ('Node31', 'Node41'), ('Node37', 'Node59'), ('Node38', 'Node59'), ('Node40', 'Node59'), ('Node40', 'Node55'), ('Node40', 'Node46'), ('Node40', 'Node42'), ('Node42', 'Node59'), ('Node42', 'Node54'), ('Node45', 'Node63'), ('Node45', 'Node61'), ('Node45', 'Node59'), ('Node45', 'Node58'), ('Node45', 'Node53'), ('Node45', 'Node51'), ('Node45', 'Node48'), ('Node47', 'Node59'), ('Node48', 'Node51'), ('Node51', 'Node63'), ('Node51', 'Node53'), ('Node52', 'Node67'), ('Node58', 'Node59'), ('Node62', 'Node65')]\n"
     ]
    }
   ],
   "source": [
    "print(graph.number_of_nodes())\n",
    "N = graph.number_of_nodes()\n",
    "print(graph.number_of_edges())\n",
    "print(list(graph.nodes))\n",
    "print(list(graph.edges))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (source,target) in list(graph.edges):\n",
    "        #print(source)\n",
    "        graph[source][target]['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if costs:\n",
    "    #change weights\n",
    "    np.random.seed(12)\n",
    "    speeds=[0.5,1.5,2.5]\n",
    "    speed_probs=[0.5,0.3,0.2]\n",
    "    speed={}\n",
    "    for s in list(graph.nodes):\n",
    "        print(s,graph.degree(s))\n",
    "        c=np.random.choice(speeds, 1, speed_probs)[0]\n",
    "        speed[s]=c\n",
    "    for (source,target) in list(graph.edges):\n",
    "            #print(source)\n",
    "            graph[source][target]['weight'] = speed[source]+speed[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 2., 2., 2.],\n",
       "       [1., 0., 1., ..., 2., 2., 2.],\n",
       "       [1., 1., 0., ..., 2., 2., 2.],\n",
       "       ...,\n",
       "       [2., 2., 2., ..., 0., 2., 2.],\n",
       "       [2., 2., 2., ..., 2., 0., 2.],\n",
       "       [2., 2., 2., ..., 2., 2., 0.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps=nx.floyd_warshall_numpy(graph)\n",
    "maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./config/resnet101-np68-Dtelekom.yaml\n",
      "bn_partitions: [68, 68, 68, 68, 68, 68, 68, 68, 68]\n",
      "budgets: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "initial_budget: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "input_partition: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "layers: [inputs, conv1.weight, layer1.0.conv1.weight, layer1.0.conv2.weight, layer1.1.conv1.weight,\n",
      "                     layer1.1.conv2.weight, layer1.2.conv1.weight, layer1.2.conv2.weight, layer2.0.conv1.weight,             layer2.0.conv2.weight, layer2.0.shortcut.0.weight,\n",
      "         layer2.1.conv1.weight, layer2.1.conv2.weight,             layer2.2.conv1.weight, layer2.2.conv2.weight, layer2.3.conv1.weight, layer2.3.conv2.weight,             layer3.0.conv1.weight, layer3.0.conv2.weight,\n",
      "         layer3.0.shortcut.0.weight, layer3.1.conv1.weight,             layer3.1.conv2.weight, layer3.2.conv1.weight, layer3.2.conv2.weight, layer3.3.conv1.weight,             layer3.3.conv2.weight, layer3.4.conv1.weight, layer3.4.conv2.weight, layer3.5.conv1.weight,             layer3.5.conv2.weight, layer3.6.conv1.weight, layer3.6.conv2.weight, layer3.7.conv1.weight,             layer3.7.conv2.weight, layer3.8.conv1.weight, layer3.8.conv2.weight, layer3.9.conv1.weight,             layer3.9.conv2.weight, layer3.10.conv1.weight, layer3.10.conv2.weight, layer3.11.conv1.weight,             layer3.11.conv2.weight, layer3.12.conv1.weight, layer3.12.conv2.weight, layer3.13.conv1.weight,             layer3.13.conv2.weight, layer3.14.conv1.weight, layer3.14.conv2.weight, layer3.15.conv1.weight,             layer3.15.conv2.weight, layer3.16.conv1.weight, layer3.16.conv2.weight, layer3.17.conv1.weight,             layer3.17.conv2.weight, layer3.18.conv1.weight, layer3.18.conv2.weight, layer3.19.conv1.weight,             layer3.19.conv2.weight, layer3.20.conv1.weight, layer3.20.conv2.weight, layer3.21.conv1.weight,             layer3.21.conv2.weight, layer3.22.conv1.weight, layer3.22.conv2.weight, layer4.0.conv1.weight,\n",
      "                     layer4.0.conv2.weight, layer4.0.shortcut.0.weight, layer4.1.conv1.weight, layer4.1.conv2.weight,             layer4.2.conv1.weight, layer4.2.conv2.weight]\n",
      "maps: [[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 3.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 3.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0], [1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 4.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 1.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0], [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0], [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0], [2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 3.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 1.0, 2.0, 3.0], [2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 2.0, 3.0, 1.0, 3.0, 2.0, 2.0, 3.0, 1.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0], [2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0], [3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 3.0, 3.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 4.0, 3.0, 2.0, 2.0, 3.0, 3.0, 0.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 1.0, 3.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 4.0], [2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 3.0, 1.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 0.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0], [2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 0.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 1.0, 2.0, 3.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 1.0], [2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 3.0, 1.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0], [3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, 2.0, 3.0, 3.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 1.0, 1.0, 3.0, 1.0, 3.0, 1.0, 3.0, 3.0, 1.0, 2.0, 1.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 1.0, 0.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0], [2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 0.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0], [3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, 2.0, 3.0, 2.0, 3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 0.0, 3.0, 2.0, 1.0, 3.0, 3.0], [3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 1.0, 3.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 3.0, 2.0, 3.0, 0.0, 3.0, 3.0, 3.0, 3.0], [2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 1.0, 2.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 0.0, 2.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 1.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 1.0, 3.0, 2.0, 0.0, 2.0, 2.0], [2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 0.0, 2.0], [2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 4.0, 2.0, 3.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 3.0, 1.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 2.0, 0.0]]\n",
      "num: 68\n"
     ]
    }
   ],
   "source": [
    "create_yaml_file(N, model_type, maps,costs=costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Node00': 0, 'Node01': 1, 'Node02': 2, 'Node03': 3, 'Node04': 4, 'Node05': 5, 'Node06': 6, 'Node07': 7, 'Node08': 8, 'Node09': 9, 'Node10': 10, 'Node11': 11, 'Node12': 12, 'Node13': 13, 'Node14': 14, 'Node15': 15, 'Node16': 16, 'Node17': 17, 'Node18': 18, 'Node19': 19, 'Node20': 20, 'Node21': 21, 'Node22': 22, 'Node23': 23, 'Node24': 24, 'Node25': 25, 'Node26': 26, 'Node27': 27, 'Node28': 28, 'Node29': 29, 'Node30': 30, 'Node31': 31, 'Node32': 32, 'Node33': 33, 'Node34': 34, 'Node35': 35, 'Node36': 36, 'Node37': 37, 'Node38': 38, 'Node39': 39, 'Node40': 40, 'Node41': 41, 'Node42': 42, 'Node43': 43, 'Node44': 44, 'Node45': 45, 'Node46': 46, 'Node47': 47, 'Node48': 48, 'Node49': 49, 'Node50': 50, 'Node51': 51, 'Node52': 52, 'Node53': 53, 'Node54': 54, 'Node55': 55, 'Node56': 56, 'Node57': 57, 'Node58': 58, 'Node59': 59, 'Node60': 60, 'Node61': 61, 'Node62': 62, 'Node63': 63, 'Node64': 64, 'Node65': 65, 'Node66': 66, 'Node67': 67}\n"
     ]
    }
   ],
   "source": [
    "mapping = {}\n",
    "i = 0\n",
    "for node in list(graph.nodes):\n",
    "    #print(node)\n",
    "    mapping[node] = i\n",
    "    i = i + 1\n",
    "print(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]\n",
      "[(0, 1), (0, 2), (0, 3), (0, 4), (0, 5), (0, 6), (1, 58), (1, 56), (1, 55), (1, 54), (1, 52), (1, 50), (1, 49), (1, 48), (1, 47), (1, 45), (1, 44), (1, 43), (1, 42), (1, 41), (1, 40), (1, 39), (1, 38), (1, 36), (1, 35), (1, 34), (1, 32), (1, 30), (1, 29), (1, 28), (1, 25), (1, 24), (1, 21), (1, 20), (1, 18), (1, 17), (1, 16), (1, 14), (1, 11), (1, 10), (1, 9), (1, 7), (1, 6), (1, 5), (1, 4), (1, 3), (1, 2), (2, 57), (2, 50), (2, 44), (2, 32), (2, 26), (2, 20), (2, 13), (2, 11), (2, 5), (2, 4), (2, 3), (3, 60), (3, 58), (3, 56), (3, 55), (3, 54), (3, 52), (3, 50), (3, 49), (3, 48), (3, 45), (3, 44), (3, 42), (3, 41), (3, 40), (3, 36), (3, 35), (3, 33), (3, 32), (3, 31), (3, 30), (3, 29), (3, 28), (3, 27), (3, 24), (3, 21), (3, 20), (3, 18), (3, 16), (3, 14), (3, 13), (3, 10), (3, 9), (3, 8), (3, 7), (3, 6), (3, 5), (3, 4), (4, 67), (4, 66), (4, 65), (4, 64), (4, 61), (4, 60), (4, 58), (4, 57), (4, 56), (4, 54), (4, 52), (4, 50), (4, 49), (4, 48), (4, 45), (4, 44), (4, 43), (4, 42), (4, 41), (4, 40), (4, 38), (4, 35), (4, 34), (4, 32), (4, 31), (4, 30), (4, 29), (4, 28), (4, 24), (4, 21), (4, 20), (4, 18), (4, 16), (4, 15), (4, 14), (4, 13), (4, 12), (4, 11), (4, 10), (4, 9), (4, 8), (4, 7), (4, 6), (4, 5), (5, 66), (5, 61), (5, 60), (5, 58), (5, 57), (5, 56), (5, 55), (5, 54), (5, 53), (5, 52), (5, 51), (5, 50), (5, 49), (5, 48), (5, 46), (5, 43), (5, 42), (5, 41), (5, 38), (5, 36), (5, 35), (5, 34), (5, 32), (5, 30), (5, 29), (5, 28), (5, 27), (5, 26), (5, 25), (5, 24), (5, 23), (5, 22), (5, 21), (5, 20), (5, 19), (5, 16), (5, 15), (5, 12), (5, 11), (5, 9), (5, 7), (5, 6), (6, 56), (6, 55), (6, 54), (6, 47), (6, 44), (6, 43), (6, 39), (6, 38), (6, 36), (6, 32), (6, 30), (6, 29), (6, 28), (6, 26), (6, 25), (6, 24), (6, 23), (6, 22), (6, 21), (6, 20), (6, 19), (6, 15), (6, 13), (6, 11), (6, 10), (6, 9), (6, 8), (6, 7), (8, 55), (8, 40), (9, 59), (9, 56), (9, 55), (9, 53), (9, 52), (9, 40), (9, 33), (9, 15), (10, 43), (11, 15), (12, 26), (15, 33), (17, 60), (17, 39), (17, 19), (18, 33), (19, 39), (23, 64), (23, 62), (23, 61), (23, 59), (23, 56), (23, 53), (23, 51), (23, 49), (23, 40), (23, 38), (23, 37), (23, 34), (23, 33), (23, 24), (26, 65), (26, 43), (26, 34), (26, 29), (27, 59), (27, 51), (28, 62), (28, 59), (31, 41), (37, 59), (38, 59), (40, 59), (40, 55), (40, 46), (40, 42), (42, 59), (42, 54), (45, 63), (45, 61), (45, 59), (45, 58), (45, 53), (45, 51), (45, 48), (47, 59), (48, 51), (51, 63), (51, 53), (52, 67), (58, 59), (62, 65)]\n"
     ]
    }
   ],
   "source": [
    "new_graph= nx.relabel_nodes(graph, mapping)\n",
    "print(list(new_graph.nodes))\n",
    "print(list(new_graph.edges))\n",
    "nodes = list(new_graph.nodes)\n",
    "edges = list(new_graph.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 1), (0, 2, 1), (0, 3, 1), (0, 4, 1), (0, 5, 1), (0, 6, 1), (1, 58, 1), (1, 56, 1), (1, 55, 1), (1, 54, 1), (1, 52, 1), (1, 50, 1), (1, 49, 1), (1, 48, 1), (1, 47, 1), (1, 45, 1), (1, 44, 1), (1, 43, 1), (1, 42, 1), (1, 41, 1), (1, 40, 1), (1, 39, 1), (1, 38, 1), (1, 36, 1), (1, 35, 1), (1, 34, 1), (1, 32, 1), (1, 30, 1), (1, 29, 1), (1, 28, 1), (1, 25, 1), (1, 24, 1), (1, 21, 1), (1, 20, 1), (1, 18, 1), (1, 17, 1), (1, 16, 1), (1, 14, 1), (1, 11, 1), (1, 10, 1), (1, 9, 1), (1, 7, 1), (1, 6, 1), (1, 5, 1), (1, 4, 1), (1, 3, 1), (1, 2, 1), (2, 57, 1), (2, 50, 1), (2, 44, 1), (2, 32, 1), (2, 26, 1), (2, 20, 1), (2, 13, 1), (2, 11, 1), (2, 5, 1), (2, 4, 1), (2, 3, 1), (3, 60, 1), (3, 58, 1), (3, 56, 1), (3, 55, 1), (3, 54, 1), (3, 52, 1), (3, 50, 1), (3, 49, 1), (3, 48, 1), (3, 45, 1), (3, 44, 1), (3, 42, 1), (3, 41, 1), (3, 40, 1), (3, 36, 1), (3, 35, 1), (3, 33, 1), (3, 32, 1), (3, 31, 1), (3, 30, 1), (3, 29, 1), (3, 28, 1), (3, 27, 1), (3, 24, 1), (3, 21, 1), (3, 20, 1), (3, 18, 1), (3, 16, 1), (3, 14, 1), (3, 13, 1), (3, 10, 1), (3, 9, 1), (3, 8, 1), (3, 7, 1), (3, 6, 1), (3, 5, 1), (3, 4, 1), (4, 67, 1), (4, 66, 1), (4, 65, 1), (4, 64, 1), (4, 61, 1), (4, 60, 1), (4, 58, 1), (4, 57, 1), (4, 56, 1), (4, 54, 1), (4, 52, 1), (4, 50, 1), (4, 49, 1), (4, 48, 1), (4, 45, 1), (4, 44, 1), (4, 43, 1), (4, 42, 1), (4, 41, 1), (4, 40, 1), (4, 38, 1), (4, 35, 1), (4, 34, 1), (4, 32, 1), (4, 31, 1), (4, 30, 1), (4, 29, 1), (4, 28, 1), (4, 24, 1), (4, 21, 1), (4, 20, 1), (4, 18, 1), (4, 16, 1), (4, 15, 1), (4, 14, 1), (4, 13, 1), (4, 12, 1), (4, 11, 1), (4, 10, 1), (4, 9, 1), (4, 8, 1), (4, 7, 1), (4, 6, 1), (4, 5, 1), (5, 66, 1), (5, 61, 1), (5, 60, 1), (5, 58, 1), (5, 57, 1), (5, 56, 1), (5, 55, 1), (5, 54, 1), (5, 53, 1), (5, 52, 1), (5, 51, 1), (5, 50, 1), (5, 49, 1), (5, 48, 1), (5, 46, 1), (5, 43, 1), (5, 42, 1), (5, 41, 1), (5, 38, 1), (5, 36, 1), (5, 35, 1), (5, 34, 1), (5, 32, 1), (5, 30, 1), (5, 29, 1), (5, 28, 1), (5, 27, 1), (5, 26, 1), (5, 25, 1), (5, 24, 1), (5, 23, 1), (5, 22, 1), (5, 21, 1), (5, 20, 1), (5, 19, 1), (5, 16, 1), (5, 15, 1), (5, 12, 1), (5, 11, 1), (5, 9, 1), (5, 7, 1), (5, 6, 1), (6, 56, 1), (6, 55, 1), (6, 54, 1), (6, 47, 1), (6, 44, 1), (6, 43, 1), (6, 39, 1), (6, 38, 1), (6, 36, 1), (6, 32, 1), (6, 30, 1), (6, 29, 1), (6, 28, 1), (6, 26, 1), (6, 25, 1), (6, 24, 1), (6, 23, 1), (6, 22, 1), (6, 21, 1), (6, 20, 1), (6, 19, 1), (6, 15, 1), (6, 13, 1), (6, 11, 1), (6, 10, 1), (6, 9, 1), (6, 8, 1), (6, 7, 1), (8, 55, 1), (8, 40, 1), (9, 59, 1), (9, 56, 1), (9, 55, 1), (9, 53, 1), (9, 52, 1), (9, 40, 1), (9, 33, 1), (9, 15, 1), (10, 43, 1), (11, 15, 1), (12, 26, 1), (15, 33, 1), (17, 60, 1), (17, 39, 1), (17, 19, 1), (18, 33, 1), (19, 39, 1), (23, 64, 1), (23, 62, 1), (23, 61, 1), (23, 59, 1), (23, 56, 1), (23, 53, 1), (23, 51, 1), (23, 49, 1), (23, 40, 1), (23, 38, 1), (23, 37, 1), (23, 34, 1), (23, 33, 1), (23, 24, 1), (26, 65, 1), (26, 43, 1), (26, 34, 1), (26, 29, 1), (27, 59, 1), (27, 51, 1), (28, 62, 1), (28, 59, 1), (31, 41, 1), (37, 59, 1), (38, 59, 1), (40, 59, 1), (40, 55, 1), (40, 46, 1), (40, 42, 1), (42, 59, 1), (42, 54, 1), (45, 63, 1), (45, 61, 1), (45, 59, 1), (45, 58, 1), (45, 53, 1), (45, 51, 1), (45, 48, 1), (47, 59, 1), (48, 51, 1), (51, 63, 1), (51, 53, 1), (52, 67, 1), (58, 59, 1), (62, 65, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "E = []\n",
    "y = (1,)\n",
    "for edge in edges:\n",
    "    edge += y\n",
    "    E.append(edge)\n",
    "print(E)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 3., 3., 3.],\n",
       "       [1., 0., 1., ..., 2., 3., 2.],\n",
       "       [1., 1., 0., ..., 3., 3., 3.],\n",
       "       ...,\n",
       "       [3., 2., 3., ..., 0., 2., 3.],\n",
       "       [3., 3., 3., ..., 2., 0., 3.],\n",
       "       [3., 2., 3., ..., 3., 3., 0.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(E)\n",
    "maps=nx.floyd_warshall_numpy(G)\n",
    "maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for (source,target) in list(G.edges):\n",
    "        #print(source)\n",
    "        G[source][target]['weight'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G[source][target]['weight'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  1, ..., 99, 99, 99],\n",
       "       [ 1,  0,  1, ..., 99, 99, 99],\n",
       "       [ 1,  1,  0, ..., 99, 99, 99],\n",
       "       ...,\n",
       "       [99, 99, 99, ...,  0, 99, 99],\n",
       "       [99, 99, 99, ..., 99,  0, 99],\n",
       "       [99, 99, 99, ..., 99, 99,  0]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#N=4\n",
    "#E=[(0,1,1),(0,2,4),(2,3,1),(0,3,1)]\n",
    "maps0 = create_maps_from_E(N,E)\n",
    "maps0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 3., 3., 3.],\n",
       "       [1., 0., 1., ..., 2., 3., 2.],\n",
       "       [1., 1., 0., ..., 3., 3., 3.],\n",
       "       ...,\n",
       "       [3., 2., 3., ..., 0., 2., 3.],\n",
       "       [3., 3., 3., ..., 2., 0., 3.],\n",
       "       [3., 2., 3., ..., 3., 3., 0.]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = [0,0,0,0.2,0.2,0.2,0.2,0.2]\n",
    "shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixsum(a,shape):\n",
    "    total=np.sum(a)\n",
    "    dif=shape-total\n",
    "    d=np.sign(dif)\n",
    "    c=0\n",
    "    for i in range(a.size):\n",
    "        if c>=np.abs(dif):\n",
    "            break\n",
    "        if a[i]!=0:\n",
    "            a[i]+=d\n",
    "            c+=1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_counts = np.round(np.array(budget) * shape).astype(int)\n",
    "a=fixsum(absolute_counts,shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, 12, 13, 13, 13, 13])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "absolute_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[123], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressions = [[[0, 0, 0 ,0], [1, 0, 0, 0]],\n",
    "                [[0, 0, 0 ,0], [1, 1, 1, 1]],\n",
    "                [[0, 0, 0 ,0], [0, 1, 1, 0]],\n",
    "                [[0, 0, 0 ,0], [0.25, 0.5, 0.75, 1]],\n",
    "                [[0, 0, 0 ,0], [0, 0.3, 1, 1]],\n",
    "                [[0, 0, 0 ,0], [0, 1, 0, 0]],\n",
    "                [[0, 0, 0 ,0], [0, 0, 1, 0]],\n",
    "                [[0, 0, 0 ,0], [0, 0, 0, 1]],\n",
    "                [[0, 0, 0 ,0], [0, 0.25, 0.25, 0.25]],\n",
    "                [[0, 0, 0 ,0], [0, 0.5, 0.5, 0.5]],\n",
    "                [[0, 0, 0 ,0], [0.5, 1, 0.75, 0.25]]\n",
    "]\n",
    "results = run_experiments(model, compressions, max_segments)\n",
    "plot_experiment_results(checkpoint, results, model_name, max_segments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressions = [\n",
    "                [[0, 0, 0 ,0], [0, 0.75, 0.75, 0.75]],\n",
    "                [[0, 0, 0 ,0], [0, 0.25, 0.25, 0.5]],\n",
    "                [[0, 0, 0 ,0], [0, 0.25, 0.25, 0.75]],\n",
    "                [[0, 0, 0 ,0], [0.25, 1, 0, 0]],\n",
    "                [[0, 0, 0 ,0], [0.25, 0, 1, 0]],\n",
    "                [[0, 0, 0 ,0], [0.25, 0, 0, 1]],\n",
    "                [[0, 0, 0 ,0], [0.25, 0, 0 ,0]],\n",
    "                [[0, 0, 0 ,0], [0.25, 0, 1, 1]],\n",
    "                [[0, 0, 0 ,0], [0.5, 1, 0, 0.25]],\n",
    "                [[0, 0, 0 ,0], [0.5, 0, 1, 0.25]],\n",
    "                [[0, 0, 0 ,0], [0.5, 0, 0, 0.25]],\n",
    "                [[0, 0, 0 ,0], [0.5, 1, 1 ,0.25]],\n",
    "                ]\n",
    "results = run_experiments(model, compressions, max_segments)\n",
    "plot_experiment_results(checkpoint, results, model_name, max_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
